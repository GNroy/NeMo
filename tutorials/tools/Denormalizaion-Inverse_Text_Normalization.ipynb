{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "velvet-trading",
   "metadata": {},
   "source": [
    "Note, this tutorial assumes both `NeMo` and `nemo_tools` packages are installed, see [`this`](https://github.com/NVIDIA/NeMo) and [`this`](https://github.com/NVIDIA/NeMo/tree/main/nemo_tools) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "\n",
    "import os\n",
    "import pynini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-listing",
   "metadata": {},
   "source": [
    "# Task Description\n",
    "\n",
    "Inverse text normalization (ITN), also called denormalization, is a part of the Automatic Speech Recognition (ASR) post-processing pipeline. \n",
    "\n",
    "ITN is the task of converting the raw spoken output of the ASR model into its written form to improve the text readability. For example, `in nineteen seventy` should be changed to `in 1975` and `one hundred and twenty three dollars` to `$123`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-words",
   "metadata": {},
   "source": [
    "# NeMo Inverse Text Normalization\n",
    "\n",
    "The NeMo ITN tool is a Python package that is based on weighted finite-state\n",
    "transducer (WFST) grammars. The tool uses [`Pynini`](https://github.com/kylebgorman/pynini) to construct WFSTs, and the created grammars can be exported and integrated into [`Sparrowhawk`](https://github.com/google/sparrowhawk) (an open-source version of [The Kestrel TTS text normalization system](https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/kestrel-tts-text-normalization-system/F0C18A3F596B75D83B75C479E23795DA)) for production. The NeMo ITN tool can be seen as a Python extension of `Sparrowhawk`. \n",
    "\n",
    "Currently, NeMo tool provides support for English and the following semiotic classes from the [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish):\n",
    "DATE, CARDINAL, MEASURE, DECIMAL, ORDINAL, MONEY, TIME, PLAIN. \n",
    "\n",
    "The toolkit is modular, easily extendable, and can be adapted to other languages and tasks like text normalization. The Python environment enables an easy combination of text covering grammars with NNs. \n",
    "\n",
    "The overall NeMo ITN pipeline from development in `Pynini` to deployment in `Sparrowhawk` is shown below:\n",
    "![alt text](deployment.png \"Inverse Text Normalization Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-radius",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "## Add ITN to your Python ASR post-processing workflow\n",
    "\n",
    "ITN is a part of the `nemo_tools` package and can be easily integrated into an existing pipeline. Installation instructions could be found [here](https://github.com/NVIDIA/NeMo/tree/main/nemo_tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_tools.text_denormalization.denormalize import denormalize\n",
    "\n",
    "raw_text = \"we paid one hundred and twenty three dollars for this desk, and this.\"\n",
    "denormalize(raw_text, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-inventory",
   "metadata": {},
   "source": [
    "In the above cell, `one hundred and twenty three dollars` would be converted to `$123`, and the rest of the words remain the same.\n",
    "\n",
    "## Run Inverse Text Normalization on an input from a file\n",
    "\n",
    "Use `run_predict.py` to convert a spoken text from a file `INPUT_FILE` to a written format and save the output to `OUTPUT_FILE`. Under the hood, `run_predict.py` is calling `denormalize()` (see the above section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update nemo_tools path\n",
    "\n",
    "NEMO_TOOLS_PATH = '/home/ebakhturina/NeMo/nemo_tools/text_denormalization'\n",
    "DATA_DIR = '/home/ebakhturina/DEL'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = f'{DATA_DIR}/test.txt'\n",
    "OUTPUT_FILE = f'{DATA_DIR}/test_itn.txt'\n",
    "\n",
    "! echo \"on march second twenty twenty\" > $DATA_DIR/test.txt\n",
    "! python $NEMO_TOOLS_PATH/run_predict.py --input=$INPUT_FILE --output=$OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the raw text was indeed converted to the written form\n",
    "! cat $OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-parish",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "\n",
    "[Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish) consists of 1.1 billion words of English text from Wikipedia, divided across 100 files. The normalized text is obtained with [The Kestrel TTS text normalization system](https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/kestrel-tts-text-normalization-system/F0C18A3F596B75D83B75C479E23795DA)).\n",
    "\n",
    "Although a large fraction of this dataset can be reused for ITN by swapping input with output, the dataset is not bijective. \n",
    "\n",
    "For example: `1,000 -> one thousand`, `1000 -> one thousand`, `3:00pm -> three p m`, `3 pm -> three p m` are valid data samples for normalization but the inverse does not hold for ITN. \n",
    "\n",
    "We used regex rules to disambiguate samples where possible, see `nemo_tools/text_denormalization/clean_eval_data.py`.\n",
    "\n",
    "To run evaluation, the `input` file should follow the Google Text normalization dataset format.\n",
    "\n",
    "Example evaluation run: \n",
    "\n",
    "`python run_evaluate.py \\\n",
    "        --input=./en_with_types/output-00001-of-00100 \\\n",
    "        [--denormalizer nemo] \\\n",
    "        [--cat CATEGORY] \\\n",
    "        [--filter]`\n",
    "        \n",
    "        \n",
    "Use `--cat` to specify a `CATEGORY` to run evaluation on (all other categories are going to be exluded from evaluation). With the option `--filter`, the provided data will be cleaned to avaid disambiguieties (use `clean_eval_data.py` to clean up the data upfront)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text = \"\"\"PLAIN\tON\t<self>\n",
    "DATE\t22 July 2012\tthe twenty second of july twenty twelve\n",
    "PUNCT\t.\tsil\n",
    "<eos>\t<eos>\n",
    "\"\"\"\n",
    "\n",
    "INPUT_FILE_EVAL = f'{DATA_DIR}/test_eval.txt'\n",
    "\n",
    "with open(INPUT_FILE_EVAL, 'w') as f:\n",
    "    f.write(eval_text)\n",
    "    \n",
    "! python $NEMO_TOOLS_PATH/run_evaluate.py --input=$INPUT_FILE_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-exemption",
   "metadata": {},
   "source": [
    "`run_evaluate.py` call will output both **sentence level** and **token level** accuracies. \n",
    "For our example, the expected output is the following:\n",
    "\n",
    "```\n",
    "Loading training data: test_eval.txt\n",
    "Sentence level evaluation...\n",
    "- Data: 1 sentences\n",
    "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 79.83it/s]\n",
    "- Deormalized. Evaluating...\n",
    "- Accuracy: 1.0\n",
    "Token level evaluation...\n",
    "- Token type: PLAIN\n",
    "  - Data: 1 tokens\n",
    "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 519.68it/s]\n",
    "  - Denormalized. Evaluating...\n",
    "  - Accuracy: 1.0\n",
    "- Token type: DATE\n",
    "  - Data: 1 tokens\n",
    "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 165.33it/s]\n",
    "  - Denormalized. Evaluating...\n",
    "  - Accuracy: 1.0\n",
    "- Accuracy: 1.0\n",
    " - Total: 2 \n",
    "\n",
    "         Class Num Tokens nemo\n",
    "0   sent level          1  1.0\n",
    "1        PLAIN          1  1.0\n",
    "2         DATE          1  1.0\n",
    "3     CARDINAL          0    0\n",
    "4      LETTERS          0    0\n",
    "5     VERBATIM          0    0\n",
    "6      MEASURE          0    0\n",
    "7      DECIMAL          0    0\n",
    "8      ORDINAL          0    0\n",
    "9        DIGIT          0    0\n",
    "10       MONEY          0    0\n",
    "11   TELEPHONE          0    0\n",
    "12  ELECTRONIC          0    0\n",
    "13    FRACTION          0    0\n",
    "14        TIME          0    0\n",
    "15     ADDRESS          0    0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-literacy",
   "metadata": {},
   "source": [
    "# C++ deployment\n",
    "\n",
    "The instructions on how to export `Pynini` grammars and to run them with `Sparrowhawk`, could be found at [NeMo/text_denormalization/tools/text_denormalization](https://github.com/NVIDIA/NeMo/tree/text_denormalization/tools/text_denormalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-nerve",
   "metadata": {},
   "source": [
    "# WFST and Common Pynini Operations\n",
    "\n",
    "Finite-state acceptor (or FSA) is a finite state automaton that has a finite number of states and no output. FSA either accepts (when the matching patter is found) or rejects a string (no match is found). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([byte for byte in bytes('fst', 'utf-8')])\n",
    "\n",
    "# create an acceptor from a string\n",
    "pynini.accep('fst')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-avatar",
   "metadata": {},
   "source": [
    "Here `0` - is a start note, `1` and `2` are the accept nodes, while `3` is a finite state.\n",
    "By default (token_type=\"byte\", `Pynini` interprets the string as a sequence of bytes, assigning one byte per arc. \n",
    "\n",
    "A finite state transducer (FST) not only matches the pattern but also produces output according to the defined transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an FST\n",
    "pynini.cross('fst', 'FST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-hypothetical",
   "metadata": {},
   "source": [
    "Pynini supports the following operations:\n",
    "\n",
    "- `closure` - Computes concatenative closure.\n",
    "- `compose` - Constructively composes two FSTs.\n",
    "- `concat` - Computes the concatenation (product) of two FSTs.\n",
    "- `difference` - Constructively computes the difference of two FSTs.\n",
    "- `invert`  - Inverts the FST's transduction.\n",
    "- `optimize` - Performs a generic optimization of the FST.\n",
    "- `project` - Converts the FST to an acceptor using input or output labels.\n",
    "- `shortestpath` - Construct an FST containing the shortest path(s) in the input FST.\n",
    "- `union`- Computes the union (sum) of two or more FSTs.\n",
    "\n",
    "\n",
    "The list of most commonly used `Pynini` operations could be found [https://github.com/kylebgorman/pynini/blob/master/CHEATSHEET](https://github.com/kylebgorman/pynini/blob/master/CHEATSHEET). \n",
    "\n",
    "Pynini examples could be found at [https://github.com/kylebgorman/pynini/tree/master/pynini/examples](https://github.com/kylebgorman/pynini/tree/master/pynini/examples).\n",
    "Use `help()` to explore the functionality. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pynini.union)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-optimum",
   "metadata": {},
   "source": [
    "# NeMo ITN API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-terrorist",
   "metadata": {},
   "source": [
    "NeMo ITN defines the following APIs that are called in sequence:\n",
    "\n",
    "- `classify()` - creates a linear automaton from the input string and composes it with the final classification WFST, which transduces numbers and inserts semantic tags.  \n",
    "- `parse()` - parses the tagged string into a list of key-value items representing the different semiotic tokens.\n",
    "- `generate_reorderings()` - takes the parsed tokens and generates string serializations with different reorderings of the key-value items. This is important since WFSTs can only process input linearly, but the word order can change from spoken to written form (e.g., `three dollars -> $3`). \n",
    "- `verbalize()` - takes the intermediate string representation and composes it with the final verbalization WFST, which removes the tags and returns the written form.  \n",
    "\n",
    "![alt text](pipeline.png \"Inverse Text Normalization Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-charles",
   "metadata": {},
   "source": [
    "# References and Further Reading:\n",
    "\n",
    "- [Ebden, Peter, and Richard Sproat. \"The Kestrel TTS text normalization system.\" Natural Language Engineering 21.3 (2015): 333.](https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/kestrel-tts-text-normalization-system/F0C18A3F596B75D83B75C479E23795DA)\n",
    "- [Gorman, Kyle. \"Pynini: A Python library for weighted finite-state grammar compilation.\" Proceedings of the SIGFSM Workshop on Statistical NLP and Weighted Automata. 2016.](https://www.aclweb.org/anthology/W16-2409.pdf)\n",
    "- [Mohri, Mehryar, Fernando Pereira, and Michael Riley. \"Weighted finite-state transducers in speech recognition.\" Computer Speech & Language 16.1 (2002): 69-88.](https://cs.nyu.edu/~mohri/postscript/csl01.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
